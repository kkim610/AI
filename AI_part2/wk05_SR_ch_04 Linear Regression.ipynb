{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Chapter 4. Linear Regression\n",
    "## text: Statistical Rethinking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'mplDeprecation' from 'matplotlib._api.deprecation' (C:\\Users\\MM\\anaconda3\\lib\\site-packages\\matplotlib\\_api\\deprecation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m griddata\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01marviz\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01maz\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:161\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _c_internal_utils\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     MatplotlibDeprecationWarning, mplDeprecation)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;129m@_api\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeprecated\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _api\u001b[38;5;241m.\u001b[39mdeprecated(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'mplDeprecation' from 'matplotlib._api.deprecation' (C:\\Users\\MM\\anaconda3\\lib\\site-packages\\matplotlib\\_api\\deprecation.py)"
     ]
    }
   ],
   "source": [
    "#import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use('arviz-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## 4.1 Why normal distribution is normal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.1.1 Normal by addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "Any process that adds together random values from the same distribution converges to\n",
    "a normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_01.png\"  width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_02.png\"  width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_03.png\"  width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_04.png\"  width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "We generate for each person a list of 16 random numbers\n",
    "between −1 and 1.   \n",
    "These are the individual steps.   \n",
    "Then we add these steps together to get\n",
    "the position after 16 steps.   \n",
    "Then we need to replicate this procedure 1000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos = np.random.uniform(-1, 1, size=(16, 1000)).sum(0)\n",
    "az.plot_kde(pos)\n",
    "plt.xlabel('position')\n",
    "plt.ylabel('Density');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.1.2 Normal by multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "1.0 means no additional growth and 1.1 means a 10%\n",
    "increase.   \n",
    "The product of all 12 is computed and returned as output.   \n",
    "Now what distribution\n",
    "do you think these random products will take?   \n",
    "Let’s generate 10,000 of them and see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.2 and 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos = np.random.uniform(1, 1.1, size=(12, 10000)).prod(0)\n",
    "az.plot_kde(pos);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos = np.random.uniform(1, 1.1, size=(12, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos.prod?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos.prod(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos.prod(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "We again get convergence towards a normal distribution, because the effect at each growth is quite small.   \n",
    "Multiplying small numbers is approximately the same as addition.   \n",
    "For\n",
    "example, if there are two numbers increasing growth by 10% each, the product is:  \n",
    "1.1 × 1.1 = 1.21  \n",
    "We could also approximate this product by just adding the increases, and be off by only 0.01:  \n",
    "1.1 × 1.1 = (1 + 0.1)(1 + 0.1) = 1 + 0.2 + 0.01 ≈ 1.2  \n",
    "The smaller the effect of each locus, the better this additive approximation will be.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "big = np.random.uniform(1, 1.5, size=(12, 10000)).prod(0)\n",
    "small = np.random.uniform(1, 1.01, size=(12, 10000)).prod(0)\n",
    "_, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "az.plot_kde(big, ax=ax[0])\n",
    "az.plot_kde(small, ax=ax[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "The interacting growth deviations, as long as they are sufficiently small, converge to a Gaussian distribution.   \n",
    "In this way, the range of casual forces that tend towards Gaussian distributions extends well beyond purely additive interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.1.3 Normal by log-multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "But wait, there’s more.   \n",
    "Large deviates that are multiplied together do not produce Gaussian distributions, but they do tend to produce Gaussian\n",
    "distributions on the log scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_big = np.log(np.random.uniform(1, 1.5, size=(12, 10000)).prod(0))\n",
    "az.plot_kde(log_big);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "So even multiplicative interactions\n",
    "of large deviations can produce Gaussian distributions, once we measure the outcomes on\n",
    "the log scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 4.2. A language for describing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**step 1:**  \n",
    "First, we recognize a set of variables that we wish to understand.   \n",
    "Some of these variables are observable.   \n",
    "We call these data.   \n",
    "Others are unobservable things like rates and averages.  \n",
    "We call these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**step 2:**  \n",
    "For each variable, we define it either in terms of the other variables or in terms of\n",
    "a probability distribution.   \n",
    "These definitions make it possible to learn about associations between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**step 3:**  \n",
    "The combination of variables and their probability distributions defines a joint generative model that can be used both to simulate hypothetical observations as well\n",
    "as analyze real ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The biggest\n",
    "difficulty usually lies in the subject matter—which variables matter and how does theory tell\n",
    "us to connect them?—not in the mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/04_05.png\"  width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "where $W$ was the observed count of water,   \n",
    "$N$ was the total number of tosses,   \n",
    "and $p$ was the proportion of water on the globe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Read the above statement as:  \n",
    "The count $W$ is distributed binomially with sample size $N$ and probability $p$.  \n",
    "The prior for $p$ is assumed to be uniform between zero and one.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Once we know the model in this way, we automatically know all of its assumptions.  \n",
    "We know the binomial distribution assumes that each sample (globe toss) is independent of the\n",
    "others, and so we also know that the model assumes that sample points are independent of\n",
    "one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For now, we'll focus on simple models like the above.   \n",
    "In these models, the first line defines the likelihood function used in Bayes’ theorem.   \n",
    "The other lines define priors.   \n",
    "Both of the lines in this model are stochastic, as indicated by the $∼$ symbol.   \n",
    "A stochastic relationship is just a mapping of a variable or parameter onto a distribution.   \n",
    "It is stochastic because no single instance of the variable on the left is known with certainty.   \n",
    "Instead, the mapping is probabilistic: Some values are more plausible than others, but very many different values are\n",
    "plausible under any model.  \n",
    "Later, we’ll have models with deterministic definitions in them as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Stochastic refers to a randomly determined process.  \n",
    "Stochastic process and random process are considered interchangeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/04_08.jpg\"  width=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "w, n = 6, 9\n",
    "p_grid = np.linspace(0, 1, 101)\n",
    "posterior = stats.binom.pmf(k=w, n=n, p=p_grid) * stats.uniform.pdf(p_grid, 0, 1)\n",
    "posterior = posterior / (posterior).sum()\n",
    "plt.plot(p_grid, posterior)\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('Density');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "p_grid = np.linspace(0, 3, 101)\n",
    "prior=stats.uniform.pdf(p_grid,1,2) # 구간 [1, 1+2] 에서 uniform distribution. 구간크기 2 * 밀도 0.5  = 1 \n",
    "plt.plot(p_grid, prior)\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('Density');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 4.3. A Gaussian model of height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 4.3.1. The data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The data contained in Howell1 are partial census data for the Dobe\n",
    "area !Kung San, compiled from interviews conducted by Nancy Howell in the late 1960s.  \n",
    "The !Kung San are the most famous foraging (수렵채집)\n",
    "population of the 20th century.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.7 and 4.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('Data/Howell1.csv', sep=';', header=0)\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "All we want for now are heights of adults in the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d2 = d[d.age >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d2=d2.reset_index(drop=True)\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "### 4.3.2. The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/04_06.png\"  width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/04_07.png\"  width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$hi ∼ Normal(µ, σ)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            [likelihood]   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$µ ∼ Normal(178, 20)$\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[µ prior]   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$σ ∼ Uniform(0, 50)$ \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "[σ prior]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "Why 178 cm?   \n",
    "Your author is 178 cm tall.   \n",
    "And the range from 138 cm to 218 cm encompasses a huge range of plausible mean heights for human populations.   \n",
    "So domain-specific\n",
    "information has gone into this prior.   \n",
    "Everyone knows something about human height and\n",
    "can set a reasonable and vague prior of this kind.   \n",
    "But in many regression problems, as you’ll\n",
    "see later, using prior information is more subtle, because parameters don’t always have such\n",
    "clear physical meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "Whatever the prior, it’s a very good idea to plot your priors, so you have a sense of the\n",
    "assumption they build into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(100, 250, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, 178, 20)); # mean=178, sigma=20 일때 p=x일 distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 60, 100)\n",
    "plt.plot(x, stats.uniform.pdf(x, 0, 50));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/04_11.png\"  width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "sample_mu = stats.norm.rvs(loc=178, scale=20, size=n_samples)\n",
    "sample_sigma = stats.uniform.rvs(loc=0, scale=50, size=n_samples)\n",
    "prior_h = stats.norm.rvs(loc=sample_mu, scale=sample_sigma)\n",
    "az.plot_kde(prior_h)\n",
    "plt.xlabel('heights')\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.15\n",
    "만일 아래와 같이 바꾸면  \n",
    "$µ ∼ Normal(178, 100)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "sample_mu = stats.norm.rvs(loc=178, scale=100, size=n_samples)\n",
    "sample_sigma = stats.uniform.rvs(loc=0, scale=50, size=n_samples)\n",
    "prior_h = stats.norm.rvs(loc=sample_mu, scale=sample_sigma)\n",
    "az.plot_kde(prior_h)\n",
    "plt.xlabel('heights')\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic/04_12.png\"  width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 4.3.3. Grid approximation of the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This isn’t the approach I encourage in any other place, because it is laborious and computationally expensive.   \n",
    "Indeed, it is usually so impractical as to be essentially impossible.   \n",
    "But as always, it\n",
    "is worth knowing what the target actually looks like, before you start accepting approximations of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data 는 130,140 이 관측 되었다.   \n",
    "prior $µ$ 는 $µ ∼ Normal(178, 20)$ 로부터 두 값 140 및 150 이 sampling 되었으며,   \n",
    "prior $\\sigma$ 는 $\\sigma ∼ Uniform(0, 50)$ 로부터 4,5,6 이 sampling 되었다 하자.     \n",
    "$likelhood ∼ Normal(µ, σ)$  이다.\n",
    "이 여섯 조합의 sampling points에 대한 posteria 를 구하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d2=d2[:2]\n",
    "d2.height=[130,140]\n",
    "d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "post = np.mgrid[140:160:0.1, 4:9:0.1].reshape(2,-1).T\n",
    "\n",
    "likelihood = [sum(stats.norm.logpdf(d2.height, loc=post[:,0][i], scale=post[:,1][i])) for i in range(len(post))]\n",
    "\n",
    "post_prod = (likelihood + \n",
    "             stats.norm.logpdf(post[:,0], loc=178, scale=20) + \n",
    "             stats.uniform.logpdf(post[:,1], loc=0, scale=50))\n",
    "post_prob = np.exp(post_prod - max(post_prod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 계산이 복잡하여 추천하지 않는다.(책의 저자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post = np.mgrid[140:160:10, 4:7:1].reshape(2,-1).T\n",
    "\n",
    "likelihood = [sum(stats.norm.logpdf(d2.height, loc=post[:,0][i], scale=post[:,1][i])) for i in range(len(post))]\n",
    "\n",
    "post_prod = (likelihood + \n",
    "             stats.norm.logpdf(post[:,0], loc=178, scale=20) + \n",
    "             stats.uniform.logpdf(post[:,1], loc=0, scale=50))\n",
    "post_prob = np.exp(post_prod - max(post_prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post[:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post[:,1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d2.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stats.norm.logpdf(130, loc=post[:,0][0], scale=post[:,1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stats.norm.logpdf(140, loc=post[:,0][0], scale=post[:,1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stats.norm.logpdf(d2.height, loc=post[:,0][0], scale=post[:,1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sum(stats.norm.logpdf(d2.height, loc=post[:,0][0], scale=post[:,1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "[sum(stats.norm.logpdf(d2.height, loc=post[:,0][i], scale=post[:,1][i])) for i in range(len(post))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "[stats.norm.logpdf(d2.height, loc=post[:,0][i], scale=post[:,1][i]) for i in range(len(post))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post_prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The strategy is the same grid approximation strategy as before (page 39). But now there are two dimensions,\n",
    "and so there is a geometric (literally) increase in bother. The algorithm is mercifully short, however, if not transparent. Think of the code as being six distinct commands. \n",
    "\n",
    "The first line of code just establishes the range of\n",
    "µ and σ values, respectively, to calculate over, as well as how many points to calculate in-between. \n",
    "The code expands those chosen µ and σ values into a matrix of all of the combinations of µ and σ. This matrix\n",
    "is stored in a data frame, post. \n",
    "\n",
    "In the second and third line of code, the log-likelihood at each combination of µ and σ is used. We have to be careful here to do everything on the log scale. Otherwise rounding error will quickly make all of the posterior probabilities zero. \n",
    "\n",
    "The second line passes the unique combination of µ and σ on each row of\n",
    "post to a function that computes the log-likelihood of each observed height, and adds all of these log-likelihoods\n",
    "together (sum). \n",
    "\n",
    "In the third line, we multiply the prior by the likelihood to get the product that is proportional\n",
    "to the posterior density. The priors are also on the log scale, and so we add them to the log-likelihood, which is\n",
    "equivalent to multiplying the raw densities by the likelihood. Finally, the obstacle for getting back on the probability scale is that rounding error is always a threat when moving from log-probability to probability. If you use\n",
    "the obvious approach, like exp( post$prod ), you’ll get a vector full of zeros, which isn’t very helpful. This\n",
    "is a result of the computer's rounding very small probabilities to zero. This is why you have to work with log-probability. \n",
    "\n",
    "The last code dodges this problem by scaling\n",
    "all of the log-products by the maximum log-product. As a result, the values in post$prob are not all zero, but\n",
    "they also aren’t exactly probabilities. Instead they are relative posterior probabilities. But that’s good enough for\n",
    "what we wish to do with these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "likelihood2 = [(stats.norm.pdf(d2.height, loc=post[:,0][i], scale=post[:,1][i])).prod() for i in range(len(post))]\n",
    "\n",
    "post_prod2 = (likelihood2 * \n",
    "             stats.norm.pdf(post[:,0], loc=178, scale=20) * \n",
    "             stats.uniform.pdf(post[:,1], loc=0, scale=50))\n",
    "post_prob2 = post_prod2 / max(post_prod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "[(stats.norm.pdf(d2.height, loc=post[:,0][i], scale=post[:,1][i])) for i in range(len(post))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "likelihood2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "type(stats.norm.pdf(d2.height, loc=post[:,0][0], scale=post[:,1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post_prod2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post_prob2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "위 두가지 방법의 계산결과 post_prob 와 post_prob2는 동일하다.     \n",
    "그러나, post_prod 와 post_prod2는 그렇지 않다.  \n",
    "prod_post2는 매우 큰 값을 갖거나, 0에 근접하는 매우 작은 값을 가질 수 있으며, 이는 round-off 에러를 발생시킬 가능성이 크다는 의미이다.  \n",
    "\n",
    "따라서, PyMC3에서는 첫번째 방법인, prior, likelihood, posteria 에서 log를 취한다. 즉, logpdf, log-likelihood 를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "원래 예제 4.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post = np.mgrid[140:160:0.1, 4:9:0.1].reshape(2,-1).T\n",
    "\n",
    "likelihood = [sum(stats.norm.logpdf(d2.height, loc=post[:,0][i], scale=post[:,1][i])) for i in range(len(post))]\n",
    "\n",
    "post_prod = (likelihood + \n",
    "             stats.norm.logpdf(post[:,0], loc=178, scale=20) + \n",
    "             stats.uniform.logpdf(post[:,1], loc=0, scale=50))\n",
    "post_prob = np.exp(post_prod - max(post_prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sum(post_prob ==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "likelihood2 = [(stats.norm.pdf(d2.height, loc=post[:,0][i], scale=post[:,1][i])).prod() for i in range(len(post))]\n",
    "\n",
    "post_prod2 = (likelihood2 * \n",
    "             stats.norm.pdf(post[:,0], loc=178, scale=20) * \n",
    "             stats.uniform.pdf(post[:,1], loc=0, scale=50))\n",
    "post_prob2 = post_prod2 / max(post_prod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post_prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sum(post_prob2 ==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "roud-off 에러로 0 이되는 값이 얼마나 많아지는지에 관한 예: 4.16에서 그리드 위치만 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post = np.mgrid[10:30:0.1, 4:10:0.1].reshape(2,-1).T\n",
    "\n",
    "likelihood = [sum(stats.norm.logpdf(d2.height, loc=post[:,0][i], scale=post[:,1][i])) for i in range(len(post))]\n",
    "\n",
    "post_prod = (likelihood + \n",
    "             stats.norm.logpdf(post[:,0], loc=178, scale=20) + \n",
    "             stats.uniform.logpdf(post[:,1], loc=0, scale=50))\n",
    "post_prob = np.exp(post_prod - max(post_prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "likelihood2 = [(stats.norm.pdf(d2.height, loc=post[:,0][i], scale=post[:,1][i])).prod() for i in range(len(post))]\n",
    "\n",
    "post_prod2 = (likelihood2 * \n",
    "             stats.norm.pdf(post[:,0], loc=178, scale=20) * \n",
    "             stats.uniform.pdf(post[:,1], loc=0, scale=50))\n",
    "post_prob2 = post_prod2 / max(post_prod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sum(post_prob ==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post_prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sum(post_prob2 ==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.17 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "You can inspect this posterior distribution, now residing in post$prob, using a variety of\n",
    "plotting commands. You can get a simple contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('Data/Howell1.csv', sep=';', header=0)\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d = d[d.age >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d=d.reset_index(drop=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post = np.mgrid[140:160:0.1, 4:9:0.1].reshape(2,-1).T\n",
    "\n",
    "likelihood = [sum(stats.norm.logpdf(d.height, loc=post[:,0][i], scale=post[:,1][i])) for i in range(len(post))]\n",
    "\n",
    "post_prod = (likelihood + \n",
    "             stats.norm.logpdf(post[:,0], loc=178, scale=20) + \n",
    "             stats.uniform.logpdf(post[:,1], loc=0, scale=50))\n",
    "post_prob = np.exp(post_prod - max(post_prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "xi = np.linspace(post[:,0].min(), post[:,0].max(), 100)\n",
    "yi = np.linspace(post[:,1].min(), post[:,1].max(), 100)\n",
    "zi = griddata((post[:,0], post[:,1]), post_prob, (xi[None,:], yi[:,None]))\n",
    "\n",
    "plt.contour(xi, yi, zi);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code  4.18 (생략)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 4.3.4. Sampling from the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.19 and 4.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sample_rows = np.random.choice(np.arange(len(post)), size=10000, replace=True, \n",
    "                               p=(post_prob/post_prob.sum()))\n",
    "sample_mu = post[:,0][sample_rows]\n",
    "sample_sigma = post[:,1][sample_rows]\n",
    "\n",
    "plt.plot(sample_mu, sample_sigma, 'o', alpha=0.05)\n",
    "plt.axis('equal')\n",
    "plt.grid(False)\n",
    "plt.xlabel('sample_mu')\n",
    "plt.ylabel('sample_sigma');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sample_rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sample_rows = np.random.choice(np.arange(len(post)), size=10000, replace=True, \n",
    "                               p=(post_prob/post_prob.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.arange(len(post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "post_prob/post_prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "p=(post_prob/post_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "0~9999는 그리드 점의 갯수를 나타내며, 각 점은 µ 1개 와 σ 1개의 쌍을 나타낸다.\n",
    "code 4.19의 첫 줄은 0~9999 까지 각각의 확률이 p 일때, size=10000개를 랜덤하게 선택\n",
    "즉, posterior probability로 부터 probability density(p로 나타냄)를 고려하여 랜덤하게 10000개 샘플 채취"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "두번째 및 세번째 줄은, 샘플로부터 µ 값과 σ값을 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "네번째 줄은 나온 횟수에 따라 색깔을 조정하여 plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "az.plot_kde(sample_mu, ax=ax[0])\n",
    "ax[0].set_xlabel('sample_mu')\n",
    "ax[0].set_yticks([])\n",
    "az.plot_kde(sample_sigma, ax=ax[1])\n",
    "ax[1].set_xlabel('sample_sigma')\n",
    "ax[1].set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.22: highest posterior density intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.hpd(sample_mu), az.hpd(sample_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.23 and  4.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For a Gaussian likelihood and a Gaussian prior on $\\mu$, the posterior distribution is always Gaussian\n",
    "as well, regardless of sample size.   \n",
    "It is the standard deviation $\\sigma$ that causes problems.  \n",
    "The deep reasons for the posterior of $\\sigma$ tending to have a long right-hand tail are complex.  \n",
    "Let’s quickly analyze only 20 of the heights from the height data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d3 = np.random.choice(d2.height, 20)\n",
    "post2 = np.mgrid[150:170:0.1, 4:20:0.1].reshape(2,-1).T\n",
    "\n",
    "likelihood2 = [sum(stats.norm.logpdf(d3, loc=post2[:,0][i], scale=post2[:,1][i])) for i in range(len(post2))]\n",
    "\n",
    "post_prod2 = (likelihood2 + \n",
    "              stats.norm.logpdf(post2[:,0], loc=178, scale=20) + \n",
    "              stats.uniform.logpdf(post2[:,1], loc=0, scale=50))\n",
    "\n",
    "post_prob2 = np.exp(post_prod2 - max(post_prod2))\n",
    "\n",
    "sample_rows2 = np.random.choice(np.arange(len(post2)), size=10000, replace=True, \n",
    "                               p=(post_prob2/post_prob2.sum()))\n",
    "sample_mu2 = post2[:,0][sample_rows2]\n",
    "sample_sigma2 = post2[:,1][sample_rows2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.plot(sample_mu2, sample_sigma2, 'o', alpha=0.05)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('sample_mu2')\n",
    "plt.ylabel('sample_sigma2')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "az.plot_kde(sample_sigma2)\n",
    "plt.xlabel('sample_sigma2')\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This code will also show a normal approximation with the same mean and variance. Now you can\n",
    "see that the posterior for $\\sigma$  is not Gaussian, but rather has a long tail of uncertainty towards higher\n",
    "values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 4.3.5  Pymc3 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.26\n",
    "\n",
    "We are repeating code 4.7, 4.8 and 4.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('Data/Howell1.csv', sep=';', header=0)\n",
    "d2 = d[d.age >= 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m4_1:\n",
    "    mu = pm.Normal('mu', mu=178, sd=20)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    height = pm.Normal('height', mu=mu, sd=sigma, observed=d2.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.28\n",
    "\n",
    "We could use a quadratic approximation like McElreath does in his book and we did in code 2.6. But Using PyMC3 is really simple to just sample from the model using a \"sampler method\". Most common sampler methods are members of the Markov Chain Monte Carlo Method (MCMC) family (for details read Section 2.4.3 and Chapter 8 of Statistical Rethinking).\n",
    "\n",
    "PyMC3 comes with various samplers. Some samplers are more suited than others for certain type of variable (and/or problems). For now we are going to let PyMC3 choose the sampler for us. PyMC3 also tries to provide a reasonable starting point for the simulation. By default PyMC3 uses the same adaptive procedure as in STAN `'jitter+adapt_diag'`, which start with a identity mass matrix and then adapt a diagonal based on the variance of the tuning samples. \n",
    "\n",
    "You can read more details of PyMC3 [here](http://pymc-devs.github.io/pymc3/notebooks/getting_started.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with m4_1:\n",
    "    trace_4_1 = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace_4_1); # this function let you check the samples values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.29\n",
    "\n",
    "Notice that compared to the table in the book we have an extra column, \"mc_error\". Since we are sampling from the posterior, there is an error introducing by the sampling process. This error can be reduced by taking more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.summary(trace_4_1, credible_interval=.89).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "testval 은 initial value를 가르킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m4_1:\n",
    "    mu = pm.Normal('mu', mu=178, sd=20, testval=d2.height.mean())\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50, testval=d2.height.std())\n",
    "    height = pm.Normal('height', mu=mu, sd=sigma, observed=d2.height)\n",
    "    trace_4_1 = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace_4_1); \n",
    "az.summary(trace_4_1, credible_interval=.11).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "mu = pm.Normal('mu', mu=178, sd=0.1)의 sd 값 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m4_2:\n",
    "    mu = pm.Normal('mu', mu=178, sd=0.1)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    height = pm.Normal('height', mu=mu, sd=sigma, observed=d2.height)\n",
    "    trace_4_2 = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace_4_2); \n",
    "az.summary(trace_4_2, credible_interval=.11).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.summary(trace_4_2, credible_interval=.89).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.32\n",
    "\n",
    "For some computations could be nice to have the trace turned into a DataFrame, this can be done using the `trace_to_dataframe` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "trace_df = pm.trace_to_dataframe(trace_4_1)\n",
    "trace_df.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.diag(trace_df.cov())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "trace_df.corr() #correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.34\n",
    "\n",
    "We did not use the quadratic approximation, instead we use a MCMC method to sample from the posterior. Thus, we already have samples. We can do something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "trace_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Or directly from the trace (we are getting the first ten samples of _sigma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "trace_4_1['sigma'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.35\n",
    "\n",
    "In our case, this is the same we did in the code 4.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.summary(trace_4_1, credible_interval=.89).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.36: multivariate sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stats.multivariate_normal.rvs(mean=trace_df.mean(), cov=trace_df.cov(), size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code extra\n",
    "\n",
    "Instead of sampling from a normal and then exponentiating to ensure sigma is positive, we can use the lognormal distribution for the same result. The Lognormal distribution is parametrized in terms of $\\tau$ (tau) the precision and not the standard deviation, where: \n",
    "\n",
    "$$tau=\\frac{1}{\\sigma^2}$$\n",
    "\n",
    "The normal distribution can also be parametrized in terms of the precision (tau). Given that the conversion between both parametrization is done right, which one to use is only a matter of convenience. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "그러나, m4-1에서 sigma는 Uniform 이었다.    \n",
    "어쨋든"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m4_1_logsigma:\n",
    "    mu = pm.Normal('mu', mu=178, sd=20)\n",
    "    sigma = pm.Lognormal('sigma', mu=2, tau=0.01)\n",
    "    height = pm.Normal('height', mu=mu, sd=sigma, observed=d2.height)\n",
    "    trace_4_1_logsigma = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace_4_1_logsigma);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 4.4  Adding a Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_13.png\"  width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.plot(d2.height, d2.weight, '.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_14.png\"  width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_15.png\"  width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.38 and 4.39\n",
    "\n",
    "Notice that the variable mu is defined as alpha + beta * d2.weight in a single line. If we want the trace to contain mu we can write as a deterministic varible. The computating will be exactly the same. The only difference is that mu will be accessible in the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m4_3:\n",
    "    alpha = pm.Normal('alpha', mu=178, sd=100)\n",
    "    beta = pm.Normal('beta', mu=0, sd=10)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    mu = alpha + beta * d2.weight\n",
    "    #mu = pm.Deterministic('mu', alpha + beta * d2.weight) # try uncomenting this line and comenting the above line\n",
    "    height = pm.Normal('height', mu=mu, sd=sigma, observed=d2.height)\n",
    "    trace_4_3 = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace_4_3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Another alternative is to write mu inside the likelihood and not as a separate line.\n",
    "\n",
    "    height = pm.Normal('height', mu=alpha + beta * d2.weight, sd=sigma, observed=d2.height)\n",
    "    \n",
    "Using PyMC3 there is not too much reason to do this. I personally think that defining mu in a separate line improves readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.summary(trace_4_3, credible_interval=.11).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "trace_df = pm.trace_to_dataframe(trace_4_3)\n",
    "trace_df.corr().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "d2 = d2.assign(weight_c=pd.Series(d2.weight - d2.weight.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m4_4:\n",
    "    alpha = pm.Normal('alpha', mu=178, sd=100)\n",
    "    beta = pm.Normal('beta', mu=0, sd=10)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    mu = alpha + beta * d2.weight_c\n",
    "    height = pm.Normal('height', mu=mu, sd=sigma, observed=d2.height)\n",
    "    trace_4_4 = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.summary(trace_4_4, credible_interval=.11).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.46\n",
    "\n",
    "Instead of using the MAP, we are going to use the mean of the posterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.plot(d2.weight, d2.height, '.')\n",
    "plt.plot(d2.weight, trace_4_3['alpha'].mean() + trace_4_3['beta'].mean() * d2.weight)\n",
    "plt.xlabel(d2.columns[1])\n",
    "plt.ylabel(d2.columns[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code  4.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pm.trace_to_dataframe(trace_4_4)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "N = [10, 50, 150, 352][0]\n",
    "with pm.Model() as m_N:\n",
    "    alpha = pm.Normal('alpha', mu=178, sd=100)\n",
    "    beta = pm.Normal('beta', mu=0, sd=10)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    mu = pm.Deterministic('mu', alpha + beta * d2.weight[:N])\n",
    "    height_hat = pm.Normal('height_hat', mu=mu, sd=sigma, observed=d2.height[:N])\n",
    "    trace_N = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "chain_N = trace_N[100:]\n",
    "az.plot_trace(chain_N, var_names='~mu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_16.png\"  width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.plot(d2.weight[:N], d2.height[:N], 'C0o')\n",
    "for _ in range(0, 20):\n",
    "    idx = np.random.randint(len(chain_N))\n",
    "    plt.plot(d2.weight[:N], chain_N['alpha'][idx] + chain_N['beta'][idx] * d2.weight[:N], 'C1-', alpha=0.5)\n",
    "plt.xlabel(d2.columns[1])\n",
    "plt.ylabel(d2.columns[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len(chain_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "위 그림은 N=10 개의 데이터를 사용하여, chain_N=900 개의 posteria points로 부터 20개를 랜덤하게 택하여, 회귀직선을 그린 그림이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Alternative we can directly use the deterministic mu variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.plot(d2.weight[:N], d2.height[:N], 'C0o')\n",
    "for _ in range(0, 20):\n",
    "    idx = np.random.randint(len(chain_N))\n",
    "    plt.plot(d2.weight[:N], chain_N['mu'][idx], 'C1-', alpha=0.5)\n",
    "plt.xlabel(d2.columns[1])\n",
    "plt.ylabel(d2.columns[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 4.4.3.4  Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.50 and 4.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "데이터를 50개 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "N = [10, 50, 150, 352][1]\n",
    "with pm.Model() as m_N:\n",
    "    alpha = pm.Normal('alpha', mu=178, sd=100)\n",
    "    beta = pm.Normal('beta', mu=0, sd=10)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    mu = pm.Deterministic('mu', alpha + beta * d2.weight[:N])\n",
    "    height_hat = pm.Normal('height_hat', mu=mu, sd=sigma, observed=d2.height[:N])\n",
    "    trace_N = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "chain_N = trace_N[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(d2.weight[:50], d2.height[:50], 'C0o')\n",
    "for _ in range(0, 20):\n",
    "    idx = np.random.randint(len(chain_N))\n",
    "    plt.plot(d2.weight[:N], chain_N['mu'][idx], 'C1-', alpha=0.5)\n",
    "plt.xlabel(d2.columns[1])\n",
    "plt.ylabel(d2.columns[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_4_4=pm.trace_to_dataframe(trace_4_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_4_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_4_4=df_4_4[::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_4_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "mu_at_50 = df_4_4[\"alpha\"] +df_4_4[\"beta\"]*(50-d2.weight.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.plot_kde(mu_at_50)\n",
    "plt.xlabel('heights')\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Code 4.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.hpd(mu_at_50, credible_interval=.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.53\n",
    "\n",
    "Using PyMC3, we do not need to compute anything else. By defining a deterministic variable mu in the model, we add that variable to the trace. Thus we get a matrix with row samples from the posterior and columns values of weights. We can access this matrix directly from the trace or turn it into a DataFrame, it all depends on what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_trace_N = pm.trace_to_dataframe(chain_N).filter(regex=('mu.*'))\n",
    "df_trace_N.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pm.summary(trace_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_trace_N = pm.trace_to_dataframe(chain_N)\n",
    "df_trace_N.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.54 and 4.58\n",
    "\n",
    "We are doing _manually_, in the book is done using the ```link``` function. In the book on code 4.58 the following operations are performed _manually_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weight_seq = np.arange(25, 71)\n",
    "# Given that we have a lot of samples we can use less of them for plotting (or we can use all!)\n",
    "chain_N_thinned = chain_N[::10]\n",
    "mu_pred = np.zeros((len(weight_seq), len(chain_N_thinned)*chain_N.nchains))\n",
    "for i, w in enumerate(weight_seq):\n",
    "    mu_pred[i] = chain_N_thinned['alpha'] + chain_N_thinned['beta'] * w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(weight_seq, mu_pred, 'C0.', alpha=0.1)\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('height');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "46은  np.arange(25, 71)로부터 왔으며, weight 25 Kg 부터 70 Kg 까지를 가르킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "chain_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "따라서 4 * 900 = 3600 개의 시뮬레이션 결과 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "chain_N_thinned = chain_N[::10] 이므로, chain_N_thinned 는 360 개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_mean = mu_pred.mean(1)\n",
    "mu_hpd = az.hpd(mu_pred.T, credible_interval=.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_hpd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_hpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(d2.weight[:N], d2.height[:N])\n",
    "plt.plot(weight_seq, mu_mean, 'k')\n",
    "az.plot_hpd(weight_seq, mu_pred.T)\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('height')\n",
    "plt.xlim(d2.weight[:N].min(), d2.weight[:N].max());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "##### Code 4.59\n",
    "\n",
    "Now we are going to use ```pm.sample_posterior_predictive``` from PyCM3. This function give us posterior predictive samples, that is for each value of the input variable we get the a sample (from the posterior) of the output variable. Thus in the following example the shape of height_pred['height_hat'].shape is (200, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "height_pred = pm.sample_posterior_predictive(chain_N, 200, m_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "height_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "height_pred.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "height_pred['height_hat'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "height_pred_hpd = az.hpd(height_pred['height_hat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "Let’s plot everything we’ve built up:   \n",
    "(1) the average line,   \n",
    "(2) the shaded region of 89% plausible µ, and   \n",
    "(3) the boundaries of the simulated heights the model expects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(d2.weight[:N], d2.height[:N])\n",
    "plt.plot(weight_seq, mu_mean, 'k')                     # (1) the average line\n",
    "az.plot_hpd(weight_seq, mu_pred.T)                     # (2) the shaded region of 89% plausible µ, and\n",
    "az.plot_hpd(d2.weight[:N], height_pred['height_hat'])  # (3) the boundaries of the simulated heights the model expects.\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('height')\n",
    "plt.xlim(d2.weight[:N].min(), d2.weight[:N].max());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.62\n",
    "\n",
    "Change the number of samples used in 4.59 (200) to other values. Because we are getting samples at the input values the jaggedness of this plot is larger than the one in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "(생략)\n",
    "#### Code 4.63\n",
    "\n",
    "Now we are going to generate heights from the posterior _manually_, instead of restricting to the input values we are going to pass an array of equally spaced weights values ```weight_seg```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "weight_seq = np.arange(25, 71)\n",
    "post_samples = []\n",
    "for _ in range(1000): # number of samples from the posterior\n",
    "    i = np.random.randint(len(chain_N))\n",
    "    mu_pr = chain_N['alpha'][i] + chain_N['beta'][i] * weight_seq\n",
    "    sigma_pred = chain_N['sigma'][i]\n",
    "    post_samples.append(np.random.normal(mu_pr, sigma_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "plt.scatter(d2.weight[:N], d2.height[:N])\n",
    "plt.plot(weight_seq, mu_mean, 'k')\n",
    "az.plot_hpd(weight_seq, mu_pred.T)\n",
    "az.plot_hpd(weight_seq, np.array(post_samples))\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('height')\n",
    "plt.xlim(d2.weight.min(), d2.weight.max());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## 4.5.1. Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_17.png\"  width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.64\n",
    "\n",
    "We have already loaded this dataset, check code 4.7 and 4.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "The first thing to do is to standardize the predictor variable.   \n",
    "We’ve done this is previous examples.   \n",
    "But this is especially helpful for working with polynomial models.   \n",
    "When predictor variables have very large values in them, there are sometimes numerical glitches(예기치 않은 고장, malfunction).  \n",
    "Even well-known statistical software can suffer from these glitches, leading to mistaken estimates.  \n",
    "These problems are very common for polynomial regression, because the square or cube of a large number can be truly\n",
    "massive.   \n",
    "Standardizing largely resolves this issue.   \n",
    "It should be your default behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_18.png\"  width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d[\"weight_std\"] = (d.weight - d.weight.mean()) / d.weight.std()\n",
    "d[\"weight_std2\"] = d.weight_std**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m_4_5:\n",
    "    alpha = pm.Normal('alpha', mu=178, sd=100)\n",
    "    beta = pm.Normal('beta', mu=0, sd=10, shape=2)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    mu = pm.Deterministic('mu', alpha + beta[0] * d.weight_std + beta[1] * d.weight_std2)\n",
    "    height = pm.Normal('height', mu=mu, sd=sigma, observed=d.height)\n",
    "    trace_4_5 = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "varnames = ['~mu']\n",
    "az.plot_trace(trace_4_5, varnames);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "az.summary(trace_4_5, varnames, credible_interval=.89).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_pred = trace_4_5['mu']\n",
    "height_pred = pm.sample_posterior_predictive(trace_4_5, 200, m_4_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(d.weight_std, d.height, c='C0', alpha=0.3)\n",
    "az.plot_hpd(d.weight_std, mu_pred, credible_interval=.89)\n",
    "az.plot_hpd(d.weight_std, height_pred['height'], credible_interval=.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "heading_collapsed": true
   },
   "source": [
    "## cubic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic/04_19.png\"  width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.70\n",
    "\n",
    "We will stack the weights to get a 2D array, these simplifies wrriting a model. Now we can compute the dot product between beta and the 2D-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weight_m = np.vstack((d.weight_std, d.weight_std**2, d.weight_std**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as m_4_6:\n",
    "    alpha = pm.Normal('alpha', mu=178, sd=100)\n",
    "    beta = pm.Normal('beta', mu=0, sd=10, shape=3)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=50)\n",
    "    mu = pm.Deterministic('mu', alpha + pm.math.dot(beta, weight_m))\n",
    "    height = pm.Normal('height', mu=mu, sd=sigma, observed=d.height)\n",
    "    trace_4_6 = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace_4_6, varnames);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu_pred = trace_4_6['mu']\n",
    "height_pred = pm.sample_posterior_predictive(trace_4_6, 200, m_4_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "source": [
    "#### Code 4.71 and 4.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(d.weight_std, d.height, c='C0', alpha=0.3)\n",
    "az.plot_hpd(d.weight_std, mu_pred, credible_interval=.89)\n",
    "az.plot_hpd(d.weight_std, height_pred['height'], credible_interval=.89)\n",
    "\n",
    "at = np.arange(-2, 3)\n",
    "plt.xticks(at, np.round(at * d.weight.std() + d.weight.mean(), 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys, IPython, scipy, matplotlib, platform\n",
    "print(\"\"\"This notebook was created using:\\nPython {}\\nIPython {}\\nPyMC3 {}\\nArviZ {}\\nNumPy {}\\nSciPy {}\\nMatplotlib {}\\n\"\"\".format(sys.version[:5], IPython.__version__, pm.__version__, az.__version__, np.__version__, scipy.__version__, matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
